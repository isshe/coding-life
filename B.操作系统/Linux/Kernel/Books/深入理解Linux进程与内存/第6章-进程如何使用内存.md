# 系统物理内存初始化

## 问题

- 程序申请内存得到的真的是物理内存吗？
- 对虚拟内存的申请如何转化为对物理内存的访问？
- 虚拟内存地址空间的起始、结束地址是什么？是连贯的吗？
- 进程占用的物理内存都是不连续的，一块一块的吗？
- top 命令输出进程的内存指标中 VIRT 和 RES 分别是什么含义？
- malloc 大致是如何工作的？
- 程序申请内存后使用内存和内核从硬盘加载 elf 文件的内容到内存，都是通过缺页中断来处理的吗？
- 缺页中断会调用用户空间的函数（do_user_addr_fault）？
- 内核将 SO、可执行文件加载到内存中的流程。
- 不同的 so，在不同的进程中，地址可能会是一样的吗？相同的 so 在不同的进程中呢？
- 进程栈和线程栈是什么？是相同的东西吗？
- 进程栈和线程栈有哪些主要区别？有哪些相同点？
- 进程栈的初始大小是多少？进程栈的大小限制是多少？这个限制可以调整吗？Linux 内核源码中哪个宏或参数控制默认大小？
- 当发生栈溢出（stack overflow）后应用程序会发生什么？
- 为何线程栈不可以动态拓展，而进程栈可以？
- 为何要在用户空间申请栈内存？
- 默认的线程栈空间是多大？如果设置了 rlimit 但是超过 32MB，那么会是多少？
- 线程栈中都存储了哪些内容？不同的分配器对于线程栈存储的内容都是一样的吗？使用方式都一样吗？
- maple tree 是什么？相比红黑树 + 双向链表有什么优势劣势？

## TODO

运行书本中的示例并查看原代码了解原理：P165、P167、P170

## 总结

### 程序申请内存得到的真的是物理内存吗？

不是直接得到物理内存。
当程序中使用 `malloc` 或其他内存分配函数分配内存时，得到的是虚拟内存地址，而不是物理内存地址，最终是通过缺页中断来分配到物理内存。
物理内存分配流程：

```
用户程序 malloc()
    ↓
glibc 内存管理器
    ↓
系统调用 (mmap/brk)
    ↓
内核虚拟内存管理 (VMA)
    ↓
缺页异常处理
    ↓
物理内存分配 (伙伴系统)
    ↓
页表映射建立
    ↓
实际物理内存
```

### 对虚拟内存的申请如何转化为对物理内存的访问？

当访问时发现虚拟内存对应的物理内存页没有分配时，会触发缺页中断，在中断中调用伙伴系统的 alloc_page 进行分配。以**页**为单位。

可参考上一个问题答案中的"物理内存分配流程"。

### 虚拟内存地址空间的起始、结束地址是什么？是连贯的吗？

虚拟内存地址空间从 0x0 开始到架构相关的最大地址结束（如 x86-64 用户空间到 0x00007FFFFFFFFFFF），但布局是不连贯的，存在多个未映射的间隙来分隔代码段、数据段、堆、mmap 区域和栈等不同内存区域。

1. **起始地址**: 通常从 0x0000000000000000 开始
2. **结束地址**:
   - x86-64: 0x00007FFFFFFFFFFF (用户空间)
   - x86-32: 0xBFFFFFFF (用户空间)
3. **连贯性**: **不连贯**，存在多个未映射的间隙
4. **布局**: 代码段 → 数据段 → 堆 → [间隙] → mmap 区域 → [间隙] → 栈
5. **随机化**: ASLR 会随机化各段的起始地址
6. **架构相关**: 不同 CPU 架构有不同的地址空间大小和布局

这种不连贯的设计是有意为之的，提供了：
- **安全性**: 间隙使得缓冲区溢出攻击更困难
- **灵活性**: 可以在间隙中动态分配内存
- **隔离性**: 不同类型的内存区域相互隔离

### 进程占用的物理内存都是不连续的，一块一块的吗？

是的，进程占用的物理内存通常都是不连续的，以页为单位（通常 4KB）分散在物理内存中，通过**页表**将这些分散的物理页映射到连续的虚拟地址空间。

### top 命令输出进程的内存指标中 VIRT 和 RES 分别是什么含义？

VIRT(Virtual Memory Size)：进程使用的**虚拟**内存总量，VIRT 包括：程序代码段 (text)、数据段 (data)、堆内存 (heap)、栈内存 (stack)、共享库映射、内存映射文件、其他虚拟内存区域。
RES(Resident Set Size)：进程使用的**物理**内存总量，RES 包括：已分配的物理页面、正在使用的内存、不包括被交换到磁盘的部分。

对比：

| 指标 | VIRT | RES |
|------|------|-----|
| **含义** | 虚拟内存总量 | 物理内存使用量 |
| **类型** | Virtual Memory | Physical Memory |
| **大小** | 通常较大 | 通常较小 |
| **变化** | 申请内存时增加 | 实际访问时增加 |
| **共享库** | 每个进程都计算 | 按实际占用计算 |
| **交换内存** | 包含 | 不包含 |

### malloc 大致是如何工作的？

malloc 通过维护空闲内存块的链表，在用户请求内存时从链表中找到合适大小的块返回，没有合适的块时通过系统调用（brk/mmap）向操作系统申请更多内存。

- 多级分配策略：根据大小选择不同的分配方法
- 空闲块管理：使用链表和分类管理空闲内存
- 内存合并：释放时合并相邻的空闲块
- 系统调用：通过 brk/sbrk 或 mmap 获取系统内存
- 性能优化：线程本地缓存、内存对齐、快速分配等

整体架构：

```
用户程序
    ↓ malloc(size)
┌─────────────────────────────────────┐
│           malloc 实现                │
│  ┌─────────────────────────────────┐ │
│  │        空闲块管理               │ │
│  │   - 空闲链表                   │ │
│  │   - 大小分类                   │ │
│  │   - 合并算法                   │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │        系统调用接口             │ │
│  │   - brk/sbrk                   │ │
│  │   - mmap                       │ │
│  └─────────────────────────────────┘ │
└─────────────────────────────────────┘
    ↓
操作系统内核
```

### 程序申请内存后使用内存和内核从硬盘加载 elf 文件的内容到内存，都是通过缺页中断来处理的吗？

是的。缺页中断有不同的类型。

**1. Minor Page Fault（次缺页中断）**

- 场景：页面在内存中，但页表项未建立
- 处理：只需更新页表，不需要磁盘 I/O
- 例子：malloc 后首次访问

**2. Major Page Fault（主缺页中断）**

- 场景：页面不在内存中，需要从磁盘读取
- 处理：需要磁盘 I/O 操作
- 例子：访问被换出的页面，或首次访问文件映射的页面

场景汇总：

| 场景 | 是否使用缺页中断 | 时机 | 类型 |
|------|------------------|------|------|
| malloc/new 申请内存 | **是** | 首次访问时 | Minor Fault |
| ELF 文件加载 | **部分是** | 取决于访问模式 | Minor/Major Fault |
| 文件 mmap | **是** | 首次访问时 | Major Fault |
| 匿名 mmap | **是** | 首次访问时 | Minor Fault |
| 静态变量 | **可能** | 程序启动时可能预加载 | Minor Fault |

### 缺页中断会调用用户空间的函数（do_user_addr_fault）？

**错误！** do_user_addr_fault 是内核空间的函数。函数名中的 "user" 不是指函数在用户空间，而是指：处理的是用户空间地址的缺页，区别于内核空间地址的缺页。

中断处理流程：

```
    用户程序访问内存
        ↓
    硬件检测缺页
        ↓
    CPU触发缺页中断 (Page Fault, 对应汇编入口 page_fault/asm_exc_page_fault)
        ↓
    切换到内核态
        ↓
    内核缺页中断处理程序
        ↓
    do_user_addr_fault() [内核函数]
        ↓
    返回用户态继续执行
```

缺页中断相关的处理有变更，详见：https://github.com/torvalds/linux/commit/91eeafea1e4b7c95cc4f38af186d7d48fceef89a

获取调用栈详见：[backtrace](./Test/第6章-进程如何使用内存/trace.sh)

### 内核将 SO、可执行文件加载到内存中的流程。

详见 [第 4 章 - 进程加载启动原理](./第4章-进程加载启动原理.md)。

完整的加载流程：

```
用户调用 execve()
        ↓
内核 do_execve()
        ↓
识别ELF格式 → load_elf_binary()
        ↓
解析ELF头部和程序头表
        ↓
映射程序段到虚拟内存
┌─────────────────────────────────────┐
│ .text段  → 只读可执行映射            │
│ .data段  → 可读写映射               │
│ .bss段   → 匿名可读写映射            │
└─────────────────────────────────────┘
        ↓
检查是否需要动态链接器
        ↓
加载动态链接器 (/lib64/ld-linux-x86-64.so.2)
        ↓
设置辅助向量和栈
        ↓
跳转到动态链接器入口点
        ↓
动态链接器工作:
├── 加载依赖的共享库 (libc.so, 等)
├── 解析符号和重定位
├── 调用初始化函数 (.init, constructors)
└── 跳转到程序main()函数
```

### 不同的 so，在不同的进程中，地址可能会是一样的吗？相同的 so 在不同的进程中呢？

相同 SO 在不同进程中的地址取决于 ASLR 设置（启用时通常不同，禁用时相同），而不同 SO 在不同进程中完全可能有相同的虚拟地址，因为每个进程都有独立的虚拟地址空间。

ASLR 默认启用，可通过 `cat /proc/sys/kernel/randomize_va_space` 确认，取值：0=禁用，1=部分启用（不包括堆），2=完全启用（默认）。

### 进程栈和线程栈是什么？是相同的东西吗？

**进程栈**（也称为**主线程栈**）是进程主线程使用的栈空间，**位于进程虚拟地址空间的栈段，通常在高地址处向下增长**。

**线程栈**是每个线程独有的栈空间，用于存储线程的局部变量、函数调用信息等。

主线程使用进程栈，而其他线程使用单独分配的线程栈。

### 进程栈和线程栈有哪些主要区别？有哪些相同点？

区别：

- 位置：进程栈是内核创建进程时申请的，位于进程地址空间的栈段；线程栈是 glibc 申请的，内核不管，通常在堆中分配
- 分配方式：进程栈由内核自动分配；线程栈由线程库（如 pthread）分配
- 扩展性：进程栈可以动态扩展（直到限制）；线程栈大小通常固定
  - 进程栈创建时只有 4KB，可以伸缩（动态拓展）；线程栈一次性申请指定大小的空间，没有伸缩功能
  - 进程栈可以伸缩、线程栈不可以伸缩
    - golang 线程栈中是可以伸缩的，golang 运行时先分配小内存，不够用后，申请大内存，并复制数据过去。
- 大小限制：进程栈受系统栈限制（ulimit -s）；线程栈有独立的大小设置。（但都受 ulimit 的 stack size 限制）

相同：

- 进程栈和线程栈都受 ulimit 中 stack size 大小的限制。可见示例：`cd 第6章-进程如何使用内存 && make test1`
- 都用于存储局部变量、函数参数、返回地址
- 都采用 LIFO（后进先出）的数据结构
- 都向低地址方向增长
- 都有栈溢出的风险

### 进程栈的初始大小是多少？进程栈的大小限制是多少？这个限制可以调整吗？Linux 内核源码中哪个宏或参数控制默认大小？

Linux 进程的进程栈初始大小是 1 页，通常是 4KB。
Linux 系统默认栈大小限制通常为 8MB。
可通过 ulimit -s 查看当前限制，最大限制（软限制）。
可通过 ulimit -s <size> 临时调整。
可通过 ulimit -Hs 查看当前最大硬限制。

在 Linux 内核源码中，相关的宏定义包括：

```c
#define _STK_LIM        (8*1024*1024)   // 默认 8MB
```

### 当发生栈溢出（stack overflow）后应用程序会发生什么？

栈溢出的常见原因：

- 递归太深：函数反复调用自身，导致调用栈越来越大
- 局部变量过大：在栈上分配了过大的数组或结构

栈溢出的后果：

- 内存访问违规 (Segmentation Fault，这是最常见的结果)：操作系统一般会为栈设置一个保护页（guard page），当栈指针越过这个保护页时，访问到未映射的地址，会触发 `段错误 (SIGSEGV)`，导致程序崩溃。
- 栈数据被破坏：未到达保护页，但是缓冲区写入越界（例如 `char buf[100]; strcpy(buf, huge_input)`），可能覆盖相邻的栈数据：
    - 局部变量被改写，导致逻辑错误。
    - 返回地址被改写，可能导致程序跳转到非法地址，崩溃，甚至被利用执行恶意代码（栈溢出漏洞的来源）。
- 不同的语言的处理不同：
  - 语言运行时异常：在 Java、Python、.NET 里，运行时一般会检测栈深度，递归过深时，抛出 StackOverflowError (Java)、RecursionError (Python) 等异常，而不是直接崩溃。
  - 未定义行为：在 C/C++ 等不做安全检查的语言里，如果溢出刚好没踩到保护页，可能只是覆盖了别的数据，程序可能继续跑下去，但会产生不可预测的结果。

### 为何线程栈不可以动态拓展，而进程栈可以？

进程主栈可以动态扩展，是因为它"独占"了虚拟地址空间的高端区域，可以安全往下长；而线程栈在共享地址空间里，只能一次性 mmap 固定大小，否则容易踩到别的线程或内存区域。

### 为何要在用户空间申请栈内存？

注意：
- 进程和线程的用户栈 都在用户空间：
    - 进程的主栈由内核在进程创建时分配；
    - 线程的栈由线程库在用户空间分配（通常通过 mmap），再传给内核。
- 每个线程还有一个内核栈（在 task_struct 中记录），仅在系统调用或中断时使用。

栈在用户空间的原因：

- 权限分离（根本原因）
  - 内核空间是特权区域，用户态代码不能随意写入。
  - 栈是用户程序的数据，理应放在用户空间。
- 高效与灵活
  - 性能：函数调用/返回只需修改栈指针，不必进入内核。
  - 虚拟内存机制：用户栈可按需增长，操作系统通过缺页异常分配新页。
  - 灵活性：用户可控制线程栈大小和位置。
- 架构一致性
  - 用户程序的数据（代码、堆、栈、全局变量）都在同一虚拟地址空间中，模型简单。
  - 硬件（MMU）天然支持用户态/内核态分离。

### 默认的线程栈空间是多大？如果设置了 rlimit 但是超过 32MB，那么会是多少？

主线程的栈大小 = ulimit -s（即 RLIMIT_STACK，通常 8MB）。
其它线程（pthread_create 创建的）默认大小是 2MB。

老版本 glibc（RHEL 6/7 那个时代）：
- 主线程：栈大小 = RLIMIT_STACK（通常 8 MB）。
- 其它线程：默认 = 2 MB，除非 RLIMIT_STACK 比 2 MB 小才跟随它。
新版本 glibc (2.27 以后)，包括你用的 Rocky 9 (glibc 2.34)：
- pthread 默认线程栈大小直接取 RLIMIT_STACK 的值（而不是强行用 2 MB）。

设置了 rlimit 但是超过 32MB 的结果：
- 主线程：rlimit 设置多少就多少，可以大于 32 MB。
- 子线程：即使 rlimit 很大，默认也最多用 32 MB；想更大，必须用 pthread_attr_setstacksize。

### 线程栈中都存储了哪些内容？不同的分配器对于线程栈存储的内容都是一样的吗？使用方式都一样吗？

存储内容：

- 局部变量：函数内定义的自动变量
- 函数参数：传递给函数的参数值
- 返回地址：函数调用完成后的返回位置
- 寄存器状态：保存的 CPU 寄存器值
- 栈帧指针：用于栈帧管理的指针（如 rbp/ebp），详见 [test4](./Test/第6章-进程如何使用内存/print_stack_info.c)
- 调用栈信息：函数调用链的上下文
- 栈保护页：用于检测栈溢出的保护区域

线程栈的基本内容在不同分配器下是相同的，因为这**主要由编译器和内核决定**。

单个栈帧的详细布局：

```
栈帧详细结构 (从高地址到低地址):

┌─────────────────────────────────────┐ <- 高地址
│          函数参数 (如果有)            │
├─────────────────────────────────────┤
│            返回地址                  │  <- call指令压入，调用者 (caller) 的操作
├─────────────────────────────────────┤
│         保存的rbp (栈帧指针)         │  <- push %rbp，被调用者 (callee) 的操作
├─────────────────────────────────────┤ <- rbp指向这里
│          局部变量1                   │
├─────────────────────────────────────┤
│          局部变量2                   │
├─────────────────────────────────────┤
│         保存的寄存器                 │  <- 如果需要保存
├─────────────────────────────────────┤
│          临时变量                    │
├─────────────────────────────────────┤
│         栈对齐填充                   │  <- 保持16字节对齐
└─────────────────────────────────────┘ <- rsp指向这里 (低地址)
```

RSP：Stack Pointer Register (栈指针寄存器)，始终指向栈顶的当前位置
RBP：Base Pointer Register (基址指针寄存器)，指向当前函数栈帧的基址（栈帧底部）

### maple tree 是什么？相比红黑树 + 双向链表有什么优势劣势？

Maple Tree 是一种 B-tree 变种，专门为内核的虚拟内存管理优化设计。在 6.1 版本引入，详见 commit：https://github.com/torvalds/linux/commit/54a611b605901c7d5d05b6b8f5d04a6ceb0962aa 和 https://github.com/torvalds/linux/commit/d4af56c5c7c6781ca6ca8075e2cf5bc119ed33d1

Maple Tree 的优势：
- 范围查询性能更好 (61% 提升)
- 内存效率更高 (节省 14% 内存)
- 缓存友好 (缓存命中率提升 11%)
- 支持 RCU 无锁读取

Maple Tree 的劣势：
- 实现复杂度高
- 调试困难
- 学习成本高
- 文档相对较少

适用场景：
- Maple Tree: 频繁范围查询、高并发读取、内存敏感场景
- 红黑树：简单键值查找、低频操作、简单场景
