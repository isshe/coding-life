# 系统物理内存初始化

## 问题

- memblock 是什么？
- 什么是固件？
- 什么是伙伴系统？
- 什么是 NUMA？
- 内核是通过什么手段来识别可用内存范围的？
- 内核管理物理内存都用了哪些技术手段？
- 为什么 free -m 展示的总内存少于 dmidecode 展示的总内存？少的内存去哪里了？
- 内核是怎么知道某个内存地址范围属于哪个 NUMA 节点的？
- 内存页管理模型的变化过程？这些模型的原理？FLAT 模型、DISCONTIG 模型、SPARSEMEM 模型。

## SPARSEMEM 模型

```c
/*
 * Permanent SPARSEMEM data:
 *
 * 1) mem_section	- memory sections, mem_map's for valid memory
 */
#ifdef CONFIG_SPARSEMEM_EXTREME
struct mem_section **mem_section;
#else
struct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT]
	____cacheline_internodealigned_in_smp;
#endif
EXPORT_SYMBOL(mem_section);
```

这是一个二维数组，引用关系如下：

struct mem_section ** -> struct mem_section * -> struct page * -> memory


## NUMA：非一致性内存访问

NUMA 全称是 Non-Uniform Memory Access，是一种针对多处理器系统的内存架构设计，旨在优化内存访问性能。其核心思想是将物理内存划分为多个节点（Node、NUMA 节点），每个节点与特定的 CPU 核心组（Socket）直接关联，形成本地内存（Local Memory），而访问其他节点的内存则称为远程内存（Remote Memory）。

**非一致性**是指不同 CPU 物理核访问不同的内存条的延迟不同。

### 获取 NUMA 信息

- Linux 内核获取 NUMA 信息：通过 firmware 层 ACPI 规范中定义的 SRAT 和 SLIT 表获取，并存储到 struct numa_meminfo。
  - SRAT：System Resource Affinity Table，用于表示 CPU 核和内存的关系图。包括有几个 Node，每个 Node 有那几个 CPU 逻辑核，有哪些内存。
  - SLIT：System Locality Information Table，记录各个 Node 之间的距离。

- 通过 numactl 获取 NUMA 信息，示例：

    ```bash
    # yum install numactl
    $ numactl --hardware
    available: 1 nodes (0)
    node 0 cpus: 0 1 2 3 4 5
    node 0 size: 15735 MB
    node 0 free: 433 MB
    node distances:
    node     0
    0:   10
    ```

## 总结

### memblock 是什么？
### 什么是固件（firmware）？
### 什么是伙伴系统？

### 什么是 NUMA？

NUMA 全称是 Non-Uniform Memory Access，是一种针对多处理器系统的内存架构设计，旨在优化内存访问性能。其核心思想是将物理内存划分为多个节点（Node、NUMA 节点），每个节点与特定的 CPU 核心组（Socket）直接关联，形成本地内存（Local Memory），而访问其他节点的内存则称为远程内存（Remote Memory）。

### 内核是通过什么手段来识别可用内存范围的？

在计算机体系中，除了操作系统和硬件外，中间还存在一层固件（firmware），起着在承上启下的作用。它对外提供的接口规范是高级配置和电源接口（ACPI，Advanced Configuration and Power Interface）。
在 ACPI 规范中，定义了探测可用内存范围的 E820 机制，操作系统在启动时，会通过 detect_memroy_e820 函数调用 ACPI 规范中定义的接口，获取到可用的物理内存地址范围。
这个探测结果可以通过 dmesg 命令输出的日志来查看。

### 内核管理物理内存都用了哪些技术手段？

主要是：memblock 初期分配器、伙伴系统、SLAB 分配器。

- memblock 初期分配器：在内核启动初期，对探测到的可用内存地址范围进行管理，这个内存分配器仅仅为了满足系统启动期间对内存页的简单管理，管理粒度较粗。
- 伙伴系统：系统启动后正常运行时采用的物理内存分配器，对外通过 alloc_pages 作为申请内存的接口。实现了 11 个空闲内存块链表，分别是 4KB、8KB ... 4MB。当内核申请内存时，伙伴系统会在自己管理的内存块中找到合适大小的内存进行分配。
- SLAB 分配器：构建在伙伴系统之上的分配器，专门为内核提供高效的小内存和对象分配（通常几十字节到几 KB）。它针对内核中频繁分配/释放的固定大小对象进行了优化，是内核内部使用的内存管理机制，不直接对用户程序开放。
  - 不对用户程序开放：用户空间程序使用的是 malloc/free 等库函数，这些通常基于 glibc 的 ptmalloc、google 的 tcmalloc 或其他分配器实现，而不是直接使用内核的 SLAB 分配器。

### 为什么 free -m 展示的总内存少于 dmidecode 展示的总内存？少的内存去哪里了？

1. 页表对象（struct page）需要占用内存。也就是假设 struct page 为 64 字节，那么管理 4KB（默认页大小）需要 64 + 4096 字节。46/4096=1.56%，因此约需 1.56% 用与内存管理，无法实际给到用户进程使用。
2. 同时，Linux 自身的运行页需要消耗内存。

### 内核是怎么知道某个内存地址范围属于哪个 NUMA 节点的？

TODO

### 内存页管理模型的变化过程？这些模型的原理？FLAT 模型、DISCONTIG 模型、SPARSEMEM 模型。
