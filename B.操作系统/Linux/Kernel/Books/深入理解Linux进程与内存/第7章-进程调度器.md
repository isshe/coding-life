# 进程调度器

## 问题

### 基础概念
- 调度器的发展过程？
- 最新内核或 5.14 及之后的调度器是什么类型的？相关的代码或宏定义有哪些？（SCHED_RR...）
- 实时进程和普通用户进程是怎么区分的？内核是如何知道的？是内核自行识别的吗？
- 进程的 nice 值的含义是什么？
- 现在的进程调度还是按时间片来执行的吗？

### 调度器架构
- 调度器是如何定义的，运行队列到底长什么样子？
- 什么是调度域（sched domain）？
- 什么是调度组（sched group）？
- CFS（完全公平调度）中，如何通过调度实体结构体（sched_entity）找到对应的进程 task_struct 的呢？

### 进程生命周期与调度
- 进程创建后，是如何加入到调度任务队列中的？
- 新进程和老进程是如何确定自己该加入哪个运行队列的？
- 调度时 CPU 核心选择顺序？
- 加入到运行队列，是否就意味着调度了呢？如果不是，是如何进行调度的呢？

### 调度时机与过程
- 调度器是何时触发选择下一个待运行的进程的，也就是任务的调度时机和检查时机？
- 调度的过程？
- 进程什么时候要让出 CPU？也就是是否需要抢占当前任务？
- 进程不主动释放 CPU 的话，每次调度最少能运行多久？最多能运行多久？
- 进程一次调度完成后，会放到哪里呢？何时再选择 CPU 进行调度呢？

### 抢占与优先级
- 在用户进程中，高优先级是否能抢占低优先级的 CPU？

### 负载均衡
- 负载均衡的过程是怎样的？负载均衡是什么时候进行的？是如何进行的？
- 怎样才算平衡呢？
- 为什么进程会在 CPU 各个核之间飘来飘去？

### 上下文切换
- 进程切换时，保存进程上下文是保存到哪里？
- 任务切换都有哪些开销？
- 进程上下文包含哪些内容？

### 高级话题
- 什么是在离线混部，业界流行的在离线混部有没有副作用？

### 实用工具
- 调度器相关的调度命令？
- taskset 命令是如何让一个进程钉在某个核上的？

## 总结

### 基础概念

#### 调度器的发展过程？

> 无论是什么调度算法，都是围绕以下两个问题：
>   - CPU 如何选择下面让哪一个任务执行？
>   - 允许选中的进程运行多长的时间？

**先进先出**：任务都加入到一个队列中，先来先服务。问题：短时任务需等待长时任务完成，系统整体的响应能力非常差。

**短作业优先**：先评估处理所需的耗时，再进行处理，时间短的优先处理。问题：不公平，长作业可能一直得不到执行。

**时间片轮转**：开始考虑公平性，每个任务都指定固定的时间，时间到执行下一个任务，任务未完成就继续进入等待队列。问题："太公平"，耗时少的紧急任务无法优先处理。

**Linux 2.4 O(n) 调度器**：同时加入动态和静态优先级，根据静态优先级和任务执行情况，计算动态优先级，避免短作业优先算法的长时任务饿死问题和时间片轮转算法中的问题。
随着 CPU 硬件在单核主频上受到物理极限限制，CPU 开始朝着多核发展，O(n) 调度器出现问题：单任务队列，锁竞争严重；O(n) 遍历方式也低效。

**Linux 2.6 O(1) 调度器**：采用多优先级（0-139 级）任务队列，同时引入 bitmap 辅助实现 O(1) 查找。
  - 优先级 0-99 给实时进程，实时进程只有静态优先级，优先级高的进程拥有绝对的优先权，并且能抢占低优先级进程的 CPU 时间。
  - 优先级 100-139 给普通进程

**完全公平调度器（CFS）**：
  - 核心思想：强调让每个进程尽量公平地分配 CPU 时间。
  - 精髓：对于 N 个进程的系统，在时间周期 T 内，每个进程运行 T/N 的时间。
  - 摒弃了固定时间片的思路，根据当前系统的情况动态计算调度周期。
  - 但绝对公平是不现实的，有的进程可能确实需要更多的 CPU，所以实际上分配是按比例来的，比例是由优先级决定的。

#### 最新内核或 5.14 及之后的调度器是什么类型的？相关的代码或宏定义有哪些？（SCHED_RR...）

```c
#define SCHED_NORMAL    0   // CFS 调度器，普通进程
#define SCHED_FIFO      1   // RT 调度器，FIFO 实时调度
#define SCHED_RR        2   // RT 调度器，Round Robin 实时调度
#define SCHED_BATCH     3   // CFS 调度器，批处理任务
#define SCHED_IDLE      5   // 空闲调度器
#define SCHED_DEADLINE  6   // DL 调度器，截止时间调度
```

在 `include/uapi/linux/sched.h` 中定义。

#### 实时进程和普通用户进程是怎么区分的？内核是如何知道的？是内核自行识别的吗？

通过 `task_struct` 的 `policy` 字段识别。

`policy` 取值：

- SCHED_NORMAL：非实时，普通进程（也叫 SCHED_OTHER），使用"完全公平策略"。
  - 对应数据结构：struct rq -> struct cfs_rq cfs
- SCHED_BATCH：非实时，批处理任务，使用"完全公平策略"。
  - 对应数据结构：struct rq -> struct cfs_rq cfs
- SCHED_IDLE：非实时，最低优先级任务（几乎闲时才运行），使用专门的"idle 调度类"。
- SCHED_FIFO：实时，先到先服务的实时任务。使用"实时调度策略"。
  - 对应数据结构：struct rq -> struct rt_rq rt
- SCHED_RR：实时，时间片轮转的实时任务。使用"实时调度策略"。
  - 对应数据结构：struct rq -> struct rt_rq rt
- SCHED_DEADLINE：实时（最强实时性），基于截止时间的最高优先级实时调度，使用"Deadline 调度器 (DL)"
  - 对应数据结构：struct rq -> struct dl_rq dl

不是内核自行识别的。是进程（程序/线程）自己主动设置或外部（系统管理员、调度器、服务管理器等）手动设置。

不进行特殊设置的话，默认是 `SCHED_NORMAL` 类型，也就是普通进程类型。

#### 进程的 nice 值的含义是什么？

nice 取值范围是 -20（最高权重） ~ 19（最低权重），默认是 0。
nice 值：进程在 CFS 调度器中获得 CPU 的权重（占用 CPU 时间的比例），不表示进程的调度优先级（也就是优先被调度）。nice 值越小则获得 CPU 的权重越高。这个权重是一个分配比例，而不是具体的时间。

实时任务（SCHED_FIFO / SCHED_RR / SCHED_DEADLINE）不受 nice 值影响，它们按优先级或 deadline 调度。

#### 现在的进程调度还是按时间片来执行的吗？

- 普通进程（CFS）：不是。通过判断 vruntime 判断是否切换。
- 实时进程：
  - SCHED_FIFO：没有时间片限制
  - SCHED_RR：仍使用固定时间片（默认 100ms，可调），同优先级的任务轮流获得 CPU
  - SCHED_DEADLINE：基于**运行时间/截止时间**的调度，不使用固定时间片，优先级由 deadline 决定。

### 调度器架构

#### 调度器是如何定义的，运行队列到底长什么样子？

Linux 调度器通过 sched_class（策略） + sched_entity（任务抽象） + per-CPU rq（运行队列）来组织。
每个运行队列中，分为实时进程和非实时进程两种，实时进程比非实时进程有更高的优先级。（详见前一个问题）
实时进程（RR）：使用优先级数组（多优先级队列）管理，取值是 0 ~ 99，数值越大优先级越高。每个优先级都用一个链表实现队列。
非实时进程（CFS）：使用红黑树管理，key 是 vruntime（虚拟运行时间）。每次调度直接选取红黑树最左边的节点即可。
实时进程（EDF）：基于 Earliest Deadline First (EDF) + Runtime/Period 参数。谁的截止时间最近，谁先运行。

#### 什么是调度域（sched domain）？

为了对应 CPU 的缓存分层结构（前面提到的 CPU 选择顺序），设计的数据结构。
调度域的层次：

```
NUMA域 (NUMA节点间)
  ↓
PKG域 (同一CPU包)
  ↓
MC域 (同一物理CPU的多核心)
  ↓
SMT域 (同一物理核心的超线程)
```

#### 什么是调度组（sched group）？

调度组是调度域内部的划分单元，用于实际的负载均衡操作。
调度组示例：（4 核 8 线程）

```
NUMA域: CPU 0-7
  └── 调度组1: CPU 0-7

PKG域: CPU 0-7
  └── 调度组1: CPU 0-7

MC域: CPU 0-7
  └── 调度组1: CPU 0-3
  └── 调度组2: CPU 4-7

SMT域:
  ├── CPU 0,4 → 调度组: CPU 0,4
  ├── CPU 1,5 → 调度组: CPU 1,5
  ├── CPU 2,6 → 调度组: CPU 2,6
  └── CPU 3,7 → 调度组: CPU 3,7
```

也就是调度域比调度组更大，调度组是调度域的子集。

#### CFS（完全公平调度）中，如何通过调度实体结构体（sched_entity）找到对应的进程 task_struct 的呢？

通过 `task_of`。实际展开其实是用的 `container_of`。例如 `container_of(se, struct task_struct, se)`。

`container_of`：根据 sched_entity 的地址和 sched_entity 在 task_struct 中的偏移地址，计算得到 task_struct 地址。

### 进程生命周期与调度

#### 进程创建后，是如何加入到调度任务队列中的？

```
- kernel_clone
  \- copy_process: 对进程进行初始化
    \- sched_fork: 调度相关的初始化
      \- _sched_fork: 初始化 task_struct.se （sched_entity），包括 vruntime 等字段都初始化为 0。
  \- wake_up_new_task: 进入到就绪队列中，等待调度器调度
    \- __set_task_cpu: 为进程选择合适的 CPU，以及指定运行队列
    \- activate_task: 将进程添加到运行队列红黑树中
```

vruntime 初始化为 0 有个问题：新进程的 vruntime 比老进程**小得多**，也就是调度优先级更高，会在比较长的时间内都占据调度优势，这不公平。
解决办法是，调度器会在每个 cfs_rq 中维护一个 min_vruntime 值，存储当前队列中所有进程的最小 vruntime。在进程真正加入运行队列时，会将 vruntime 设置为 min_vruntime。

#### 新进程和老进程是如何确定自己该加入哪个运行队列的？

内核会根据 CPU 亲和性、NUMA 拓扑、cache 层次 和 负载均衡 来决定目标 CPU。

上一次使用的 CPU -> 与唤醒该进程的进程（唤醒者）相同的 CPU（当前逻辑核）-> 共享缓存且 IDLE 的 CPU -> 负载最小的 CPU。

- 上一次使用的 CPU → 对应 cache 热亲和
- 与唤醒进程相同 CPU → 对应 wake_affine 策略
- 共享缓存且 IDLE 的 CPU → 对应 NUMA/LLC 亲和 + 空闲优先
- 负载最小的 CPU → 对应 调度域负载均衡

#### 调度时 CPU 核心选择顺序？

根据调度域层级来选择的。从调度域底层往上进行选择。选择优先级/顺序如下：

- 当前 CPU / 上次运行的 CPU：共享所有缓存
- 同一物理核心的其他超线程：共享 L1、L2 缓存
- 同一处理器芯片：共享 L3 缓存和内存控制器
- 同一 NUMA 节点的其他处理器芯片：共享本地内存、避免跨 NUMA 访问
- 跨 NUMA 节点访问

#### 加入到运行队列，是否就意味着调度了呢？如果不是，是如何进行调度的呢？

不是，还需要等待调度器进行调度。

具体何时被调度，详见问题"调度器是何时触发选择下一个待运行的进程的，也就是任务的调度时机和检查时机？"。

### 调度时机与过程

#### 调度器是何时触发选择下一个待运行的进程的，也就是任务的调度时机和检查时机？

任务调度/执行主要有 3 个时机：

- **调度节拍（scheduler tick）**：周期性时钟中断会调用 scheduler_tick()，更新当前任务的 vruntime，并检查是否需要切换到更合适的任务；同时根据调度周期，也可能触发负载均衡，将部分任务迁移到本核或其他核。
- **主动放弃 CPU**：任务在阻塞/睡眠、调用 sched_yield() 或退出（exit()）时，会主动调用 schedule()，此时调度器选择下一个任务。
- **异步抢占**：当更高优先级任务被唤醒，或者在内核可抢占点、中断返回用户态时，如果发现 TIF_NEED_RESCHED 被设置，也会立即触发调度。

检查时机：

- **时钟中断 tick**：周期性检查当前进程是否需要被抢占，通常每毫秒触发一次（HZ=1000 时），相关函数 scheduler_tick()
- **进程唤醒时**：新唤醒的进程可能抢占当前运行的进程，时机是任何进程从阻塞状态变为可运行状态时，相关函数 try_to_wake_up() → check_preempt_curr()
- **系统调用/中断返回时**：从内核态返回用户态前检查是否需要调度，时机是每次系统调用完成或中断处理完成时，相关函数 syscall_return_slowpath() / irq_exit()
- **内核抢占点（CONFIG_PREEMPT 启用时）**：在内核执行过程中的安全点检查抢占，时机是内核代码中的显式抢占检查点，相关函数 preempt_schedule() / might_sleep()

#### 调度的过程？

调度是在 schedule 函数中进行的，其中核心函数是 schedule 中调用的 __schedule 函数。

调度过程：

- 获取当前 CPU ID（smp_processor_id）
- 获取当前 CPU 的任务队列（cpu_rq）
- 从任务队列中选择一个任务（pick_next_task）
- 进行上下文切换（context_switch），将新进程切换到运行状态
  - 保存老进程上下文
  - 执行地址空间切换（switch_mm_irqs_off）
  - 执行栈和寄存器切换（switch_to）

#### 进程什么时候要让出 CPU？也就是是否需要抢占当前任务？

**主动让出**：

- 调用 sleep()、wait() 等阻塞函数
- 等待 I/O 完成
- 等待锁、信号量等资源
- 主动调用 yield()

**被动抢占**：

- 高优先级进程唤醒（RT > CFS）
- 时间片用完（RT_RR 和 CFS）
- CFS 中 vruntime 差距过大，相关逻辑在 `check_preempt_tick` 函数中。
- DL 任务的 deadline 更紧迫

#### 进程不主动释放 CPU 的话，每次调度最少能运行多久？最多能运行多久？

**普通进程（CFS 调度类 SCHED_NORMAL / SCHED_OTHER）**：
- 最少能运行多久：不是固定的时间片，而是直到有更高优先级（更小 vruntime）的进程到来。
- 最多能运行多久：CFS 会根据 sched_latency_ns（默认大约 6ms~24ms，取决于负载）和 min_granularity_ns（最小调度粒度，默认 ~0.75ms）计算出每个进程的**公平份额**。所以最多运行时间 ≈ max(vruntime_slice, min_granularity_ns)，通常在**几毫秒到十几毫秒**级别。

用以下命令获取：（不一定会有）

```bash
sysctl -a | grep sched_min_granularity_ns
```

**实时进程（RT 调度类 SCHED_FIFO / SCHED_RR）**：
- SCHED_FIFO：没有时间片限制，不自己让出会一直运行。
- SCHED_RR（Round-Robin 时间片轮转）：有固定的时间片（默认 100ms，可通过 /proc/sys/kernel/sched_rr_timeslice_ms 配置），最长一个时间片。

用以下命令获取：

```bash
sysctl -a | grep sched_rr_timeslice_ms
```

#### 进程一次调度完成后，会放到哪里呢？何时再选择 CPU 进行调度呢？

执行完成后进程的去向取决于进程的状态：

- 时间片用完但还能运行 → 放回运行队列末尾，等待下次调度
- 需要等待 I/O 或资源 → 放入等待队列（struct wait_queue_head），暂时不参与调度
- 进程结束 → 进程终止，释放资源

重新获得 CPU 的时机：

- 轮到它在运行队列中被选中（如果在运行队列）
- 等待的资源就绪时被唤醒（如果在等待队列）

### 抢占与优先级

#### 在用户进程中，高优先级是否能抢占低优先级的 CPU？

**普通进程**：CFS 是"软抢占"，低 nice 的 CFS 进程不保证立即抢占高 nice 的 CFS 进程，而是通过 vruntime 进行调度，但是 nice 不同则 vruntime 增长速度会不同。
**实时进程**：高优先级 RT 进程可以立即抢占低优先级 RT 或 CFS 进程。

优先级：stop task > deadline task > RT task > CFS task

### 负载均衡

#### 负载均衡的过程是怎样的？负载均衡是什么时候进行的？是如何进行的？

注意：负载均衡的单位是调度组，不是调度域。

在调度节拍中定时发起的；在 scheduler_tick 函数中调用 trigger_load_balance 来触发，trigger_load_balance 则通过触发一个软中断来让 ksoftirqd 线程处理真正的负载均衡过程。
软中断是 SCHED_SOFTIRQ，在系统初始化的时候就已经设置好中断处理函数 —— run_rebalance_domains（完全公平调度器的）。
负载均衡的过程是：从最底层开始遍历每一层调度域，优先尝试在最底层调度域实现负载均衡，如果无法实现，再逐级往上。（因为前面提到的缓存命中的问题）。负载均衡的过程在 rebalance_domains 中。
- 选择调度域：从最底层调度域开始
- 当前 CPU 负载检查：判断当前 CPU 是否有余力处理更多任务
- 查找最忙的调度组：在该域中找到负载最重的组
- 选择源队列：从最忙组中找到最忙的队列
- 迁移任务：将任务从源 CPU 迁移/拉取（detach_tasks）到当前 CPU

注意：并不是所有任务都能迁移，例如通过 taskset 或调用 sched_setaffinity 的则不能迁移。是否能迁移通过 can_migrate_task 判断。

#### 怎样才算平衡呢？

基本平衡条件：各 CPU 之间的负载差距在可接受范围内。不追求绝对的平衡。

判断指标：

- 负载权重差距。CFS 负载权重计算
  - 每个进程有负载权重（基于 nice 值）
  - CPU 负载 = 该 CPU 上所有进程的权重总和
  - 平衡目标：各 CPU 负载权重尽可能接近
- 运行队列长度差距
- CPU 利用率差距

具体的判断方式可以看 Linux 源码中 calculate_imbalance 函数以及 struct sg_lb_stats（update_sd_lb_stats）相关内容。

注意：每个调度域的平衡标准不同，例如 SMT 域（超线程）是超线程平衡，MC 域（多核）是各个核心平衡。

#### 为什么进程会在 CPU 各个核之间飘来飘去？

因为有的队列比较忙，有的比较闲，进行负载均衡时，可能会导致进程在 CPU 各个核之间飘来飘去。

### 上下文切换

#### 进程切换时，保存进程上下文是保存到哪里？

- **内核栈（Kernel Stack）**：最主要的保存位置，每个进程都有独立的内核栈（struct thread_struct）。
  - CPU 寄存器状态（通过 pt_regs 结构）
  - 系统调用参数
  - 中断处理信息
  - 内核函数调用栈
- **task_struct 结构体**：进程控制块，存储进程的核心状态信息
- **寄存器上下文（pt_regs）**：保存在内核栈顶部，存储用户态寄存器。
- **内存映射信息（mm_struct）**：独立存储，描述进程的虚拟内存空间

相关代码在 arch/x86/kernel/process_64.c 文件的 __switch_to()。

#### 任务切换都有哪些开销？

可以分为两种开销：直接开销和间接开销。

**直接开销**：切换过程中，必须要做的事情，例如地址空间、栈、寄存器切换等。
**间接开销**：缓存还不热，刚开始运行速度较慢。

#### 进程上下文包含哪些内容？

包含地址空间、栈、寄存器等。

### 高级话题

#### 什么是在离线混部，业界流行的在离线混部有没有副作用？

在离线混部（Online-Offline Co-location）是指在同一个计算集群或服务器上同时运行在线服务（对延迟敏感的实时业务）和离线任务（批处理、数据分析等对延迟不敏感的任务）的技术方案。

**在线服务**：如 Web 服务、API 接口、实时推荐等，要求低延迟、高可用
**离线任务**：如数据处理、机器学习训练、报表生成等，可以容忍较高延迟

**副作用**：导致 wake_affine 机制失效，进程在不同的核心上运行的概率增加，Cache 是凉的，穿透到内存的访问次数增加，进而导致进程的运行性能下降。wake_affine 就是内核 CFS（完全公平调度器）的一种优化机制，尝试让被唤醒的进程 B 尽量被调度到唤醒它的那个 CPU（或者同 NUMA 节点/同 LLC 共享 cache 的 CPU）。

### 实用工具

#### 调度器相关的调度命令？

**chrt**：查看或修改进程的调度策略。详见[chrt 命令](../../../Commands/chrt.md)

```bash
sudo chrt -p 12345
```

**nice**：设置进程的 nice 值。nice 值越低占用越多 CPU。（和人类似，人越 nice 越倾向让出资源；越不 nice 越倾向抢夺资源）。详见[nice 命令](../../../Commands/nice.md)

```bash
nice -n -20 <PROGRAM_NAME>
```

**renice**：设置**正在运行**的进程的 nice 值。详见[renice 命令](../../../Commands/renice.md)

```bash
renice +5 <PID>
```

**taskset**：设置进程的调度亲缘性。详见[taskset 命令](../../../Commands/taskset.md)

#### taskset 命令是如何让一个进程钉在某个核上的？

通过调用 Linux 内核的 CPU 亲和性（CPU affinity）系统调用来实现进程与特定 CPU 核心的绑定。

设置亲和性的系统调用：`sched_setaffinity()`
获取亲和性的系统调用：`sched_getaffinity()`
