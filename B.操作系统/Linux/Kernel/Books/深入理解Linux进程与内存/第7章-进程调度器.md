# 进程调度器

## 问题

- 调度器的发展过程？
  - 无论是什么调度算法，都是围绕以下两个问题：
    - CPU 如何选择下面让哪一个任务执行？
    - 允许选中的进程运行多长的时间？
- 实时进程和普通用户进程是怎么区分的？内核是如何知道的？是内核自行识别的吗？
- 调度器是如何定义的，运行队列到底长什么样子？
- 新进程和老进程是如何确定自己该加入哪个运行队列的？
- 调度器是何时触发选择下一个待运行的进程的？
- 进程不主动释放 CPU 的话，每次调度最少能运行多久？最多能运行多久？
- 现在的进程调度还是按时间片来执行的吗？
- 进程的 nice 值的含义是什么？
- 在用户进程中，高优先级是否能抢占低优先级的 CPU？
- 什么是在离线混部，业界流行的在离线混部有没有副作用？
- 为什么进程会在 CPU 各个核之间飘来飘去？
- taskset 命令是如何让一个进程钉在某个核上的？
- 最新内核或 5.14 及之后的调度器是什么类型的？相关的代码或宏定义有哪些？（SCHED_RR...）
- CFS（完全公平调度）中，如何通过调度实体结构体（sched_entity）找到对应的进程 task_struct 的呢？
- 进程创建后，是如何加入到调度任务队列中的？
- 一个进程在某个 CPU 调度过后，还能迁移到其他 CPU 上执行吗？
- 调度时 CPU 核心选择顺序？
  - 根据缓存进行选择：同一个物理核（2 个逻辑核，L1/L2 共享）-> 同一个物理 CPU（L3 共享） -> 不同物理 CPU。
- 进程一次调度完成后，会放到哪里呢？何时再选择 CPU 进行调度呢？
- 加入到运行队列，是否就意味着调度了呢？如果不是，是如何进行调度的呢？
- 进程什么时候要让出 CPU？也就是是否需要抢占当前任务？
- 什么是调度域（sched domain）？
- 什么是调度组（sched group）？
- 负载均衡的过程是怎样的？负载均衡是什么时候进行的？
- 怎样才算平衡呢？
- 调度的过程？
- 什么时候会触发调度/上下文切换？
- 保存进程上下文时，都保存到哪里？
- 任务切换都有哪些开销？
- 进程上下文包含哪些内容？
- 调度器相关的调度命令？

## 总结

### 调度器的发展过程？

> 无论是什么调度算法，都是围绕以下两个问题：
>   - CPU 如何选择下面让哪一个任务执行？
>   - 允许选中的进程运行多长的时间？

先进先出：任务都加入到一个队列中，先来先服务。问题：短时任务需等待长时任务完成，系统整体的响应能力非常差。
->
短作业优先：先评估处理所需的耗时，再进行处理，时间短的优先处理。问题：不公平，长作业可能一直得不到执行。
->
时间片轮转：开始考虑公平性，每个任务都指定固定的时间，时间到执行下一个任务，任务未完成就继续进入等待队列。问题：“太公平”，耗时少的紧急任务无法优先处理。
->
Linux 2.4 O(n) 调度器：同时加入动态和静态优先级，根据静态优先级和任务执行情况，计算动态优先级，避免短作业优先算法的长时任务饿死问题和时间片轮转算法中的问题。
随着 CPU 硬件在单核主频上受到物理极限限制，CPU 开始朝着多核发展，O(n) 调度器出现问题：单任务队列，锁竞争严重；O(n) 遍历方式也低效。
->
Linux 2.5 O(1) 调度器：采用多优先级（0-139 级）任务队列，同时引入 bitmap 辅助实现 O(1) 查找。
  - 优先级 0-99 给实时进程，实时进程只有静态优先级，优先级高的进程拥有绝对的优先权，并且能抢占低优先级进程的 CPU 时间。
  - 优先级 100-140 给普通进程
-> 完全公平调度器：
  - 核心思想：强调让每个进程尽量公平地分配 CPU 时间。
  - 精髓：对于 N 个进程的系统，在时间周期 T 内，每个进程运行 T/N 的时间。
  - 摒弃了固定时间片的思路，根据当前系统的情况动态计算调度周期。
  - 但绝对公平是不现实的，有的进程可能确实需要更多的 CPU，所以实际上分配是按比例来的，比例是由优先级决定的。

### 实时进程和普通用户进程是怎么区分的？内核是如何知道的？是内核自行识别的吗？

通过 `task_struct` 的 `policy` 字段识别。

`policy` 取值：

- SCHED_OTHER：非实时，普通进程（也叫 SCHED_NORMAL），使用“完全公平策略”。
- SCHED_BATCH：非实时，批处理任务，使用“完全公平策略”。
- SCHED_IDLE：非实时，最低优先级任务（几乎闲时才运行），使用“完全公平策略”。
- SCHED_FIFO：实时，先到先服务的实时任务。使用“实时调度策略”。
- SCHED_RR：实时，时间片轮转的实时任务。使用“实时调度策略”。
- SCHED_DEADLINE：实时（最强的实时性），基于截止时间的实时任务（EDF）。使用“Deadline 调度策略”。
  - 对应数据结构：struct rq -> struct dl_rq dl

不是内核自行识别的。是进程（程序/线程）自己主动设置或外部（系统管理员、调度器、服务管理器等）手动设置。

不进行特殊设置的话，默认是 `SCHED_OTHER` 类型，也就是普通进程类型。

### 调度器是如何定义的，运行队列到底长什么样子？

Linux 调度器通过 sched_class（策略） + sched_entity（任务抽象） + per-CPU rq（运行队列）来组织。
每个运行队列中，分为实时进程和非实时进程两种，实时进程比非实时进程有更高的优先级。（详见前一个问题）
实时进程（RR）：使用优先级数组（多优先级队列）管理，取值是 0 ~ 99，数值越大优先级越高。每个优先级都用一个链表实现队列。
非实时进程（CFS）：使用红黑树管理，key 是 vruntime（虚拟运行时间）。每次调度直接选取红黑树最左边的节点即可。
实时进程（EDF）：基于 Earliest Deadline First (EDF) + Runtime/Period 参数。谁的截止时间最近，谁先运行。

### 新进程和老进程是如何确定自己该加入哪个运行队列的？

内核会根据 CPU 亲和性、NUMA 拓扑、cache 层次 和 负载均衡 来决定目标 CPU。

上一次使用的 CPU -> 与唤醒该进程的进程（唤醒者）相同的 CPU（当前逻辑核）-> 共享缓存且 IDLE 的 CPU -> 负载最小的 CPU。

- 上一次使用的 CPU → 对应 cache 热亲和
- 与唤醒进程相同 CPU → 对应 wake_affine 策略
- 共享缓存且 IDLE 的 CPU → 对应 NUMA/LLC 亲和 + 空闲优先
- 负载最小的 CPU → 对应 调度域负载均衡

### 调度器是何时触发选择下一个待运行的进程的？

任务调度主要有 3 个时机：（书中写的是 2 个时机，这里补充“抢占”）

- 调度节拍（scheduler tick）：周期性时钟中断会调用 scheduler_tick()，更新当前任务的 vruntime，并检查是否需要切换到更合适的任务；同时根据调度周期，也可能触发负载均衡，将部分任务迁移到本核或其他核。
- 主动放弃 CPU：任务在阻塞/睡眠、调用 sched_yield() 或退出（exit()）时，会主动调用 schedule()，此时调度器选择下一个任务。
- 异步抢占：当更高优先级任务被唤醒，或者在内核可抢占点、中断返回用户态时，如果发现 TIF_NEED_RESCHED 被设置，也会立即触发调度。

### 进程不主动释放 CPU 的话，每次调度最少能运行多久？最多能运行多久？

普通进程（CFS 调度类 SCHED_NORMAL / SCHED_OTHER）：
- 最少能运行多久：不是固定的时间片，而是 直到有更高优先级（更小 vruntime）的进程到来。
- 最多能运行多久：CFS 会根据 sched_latency_ns（默认大约 6ms~24ms，取决于负载）和 min_granularity_ns（最小调度粒度，默认 ~0.75ms）计算出每个进程的**公平份额**。所以最多运行时间 ≈ max(vruntime_slice, min_granularity_ns)，通常在**几毫秒到十几毫秒**级别。

用以下命令获取：（不一定会有）

```bash
sysctl -a | grep sched_min_granularity_ns
```

实时进程（RT 调度类 SCHED_FIFO / SCHED_RR）：
- SCHED_FIFO：没有时间片限制，不自己让出会一直运行。
- SCHED_RR（Round-Robin 时间片轮转）：有固定的时间片（默认 100ms，可通过 /proc/sys/kernel/sched_rr_timeslice_ms 配置），最长一个时间片。

用以下命令获取：

```bash
sysctl -a | grep sched_rr_timeslice_ms
```

### 现在的进程调度还是按时间片来执行的吗？

- 普通进程（CFS）：不是。通过判断 vruntime 判断是否切换。
- 实时进程：
  - SCHED_FIFO：没有时间片限制
  - SCHED_RR：仍使用固定时间片（默认 100ms，可调），同优先级的任务轮流获得 CPU
  - SCHED_DEADLINE：基于**运行时间/截止时间**的调度，不使用固定时间片，优先级由 deadline 决定。

### 进程的 nice 值的含义是什么？
nice 值表示：

nice 取值范围是 -20（最高权重） ~ 19（最低权重），默认是 0。
nice 值：进程在 CFS 调度器中获得 CPU 的权重（占用 CPU 时间的比例），不表示进程的调度优先级（也就是优先被调度）。nice 值越小则获得 CPU 的权重越高。这个权重是一个分配比例，而不是具体的时间。

实时任务（SCHED_FIFO / SCHED_RR / SCHED_DEADLINE）不受 nice 值影响，它们按优先级或 deadline 调度。

### 在用户进程中，高优先级是否能抢占低优先级的 CPU？

普通进程：CFS 是“软抢占”，低 nice 的 CFS 进程不保证立即抢占高 nice 的 CFS 进程，而是通过 vruntime 进行调度，但是 nice 不同则 vruntime 增长速度会不同。
实时进程：高优先级 RT 进程可以立即抢占低优先级 RT 或 CFS 进程。

优先级：stop task > deadline task > RT task > CFS task

### 什么是在离线混部，业界流行的在离线混部有没有副作用？



### CFS（完全公平调度）中，如何通过调度实体结构体（sched_entity）找到对应的进程 task_struct 的呢？

通过 `task_of`。实际展开其实是用的 `container_of`。例如 `container_of(se, struct task_struct, se)`。

`container_of`：根据 sched_entity 的地址和 sched_entity 在 task_struct 中的偏移地址，计算得到 task_struct 地址。


### 进程创建后，是如何加入到调度任务队列中的？

```
- kernel_clone
  \- copy_process: 对进程进行初始化
    \- sched_fork: 调度相关的初始化
      \- _sched_fork: 初始化 task_struct.se （sched_entity），包括 vruntime 等字段都初始化为 0。
  \- wake_up_new_task: 进入到就绪队列中，等待调度器调度
    \- __set_task_cpu: 为进程选择合适的 CPU，以及指定运行队列
    \- activate_task: 将进程添加到运行队列红黑树中
```

vruntime 初始化为 0 有个问题：新进程的 vruntime 比老进程**小得多**，也就是调度优先级更高，会在比较长的时间内都占据调度优势，这不公平。
解决办法是，调度器会在每个 cfs_rq 中维护一个 min_vruntime 值，存储当前队列中所有进程的最小 vruntime。在进程真正加入运行队列时，会将 vruntime 设置为 min_vruntime。

### 进程什么时候要让出 CPU？也就是是否需要抢占当前任务？

以下两种情况需要：

- 当前仅实际运行的时间比根据调度周期算出的时间预期运行时间长了（也就是运行太久了，运行时间到了）
- 当前进程的 vruntime 已经比红黑树最左侧（vruntime 最小）的任务的 vruntime 大不少了。
  - **不少**具体是多少？sched_slice 计算出来的 ideal runtime（理想运行时间）

相关逻辑在 `check_preempt_tick` 函数中。

### 什么是调度域？

为了对应 CPU 的缓存分层结构（前面提到的 CPU 选择顺序），设计的数据结构。
调度域的层次：

```
NUMA域 (NUMA节点间)
  ↓
PKG域 (同一CPU包)
  ↓
MC域 (同一物理CPU的多核心)
  ↓
SMT域 (同一物理核心的超线程)
```

### 什么是调度组（sched group）？

调度组是调度域内部的划分单元，用于实际的负载均衡操作。
调度组示例：（4 核 8 线程）

```
NUMA域: CPU 0-7
  └── 调度组1: CPU 0-7

PKG域: CPU 0-7
  └── 调度组1: CPU 0-7

MC域: CPU 0-7
  └── 调度组1: CPU 0-3
  └── 调度组2: CPU 4-7

SMT域:
  ├── CPU 0,4 → 调度组: CPU 0,4
  ├── CPU 1,5 → 调度组: CPU 1,5
  ├── CPU 2,6 → 调度组: CPU 2,6
  └── CPU 3,7 → 调度组: CPU 3,7
```

也就是调度域比调度组更大，调度组是调度域的子集。

### 负载均衡的过程是怎样的？负载均衡是什么时候进行的？是如何进行的？

注意：负载均衡的单位是调度组，不是调度域。

在调度节拍中定时发起的；在 scheduler_tick 函数中调用 trigger_load_balance 来触发，trigger_load_balance 则通过触发一个软中断来让 ksoftirqd 现成处理真正的负载均衡过程。
软中断是 SCHED_SOFTIRQ，在系统初始化的时候就已经设置好中断处理函数 —— run_rebalance_domains（完全公平调度器的）。
负载均衡的过程是：从最底层开始遍历每一层调度域，优先尝试在最底层调度域实现负载均衡，如果无法实现，再逐级往上。（因为前面提到的缓存命中的问题）。负载均衡的过程在 rebalance_domains 中。
- 选择调度域：从最底层调度域开始
- 当前 CPU 负载检查：判断当前 CPU 是否有余力处理更多任务
- 查找最忙的调度组：在该域中找到负载最重的组
- 选择源队列：从最忙组中找到最忙的队列
- 迁移任务：将任务从源 CPU 迁移/拉取（detach_tasks）到当前 CPU

注意：并不是所有任务都能拉取，例如通过 taskset 或调用 sched_setaffinity 的则不能拉。是否能拉取通过 can_migrate_task 判断。

### 怎样才算平衡呢？

TODO

### 调度的过程？

调度 是在 schedule 函数中进行的，其中核心函数是 schedule 中调用的 __schedule 函数。

调度过程：

- 获取当前 CPU ID（smp_processor_id）
- 获取当前 CPU 的任务队列（cpu_rq）
- 从任务队列中选择一个任务（pick_next_task）
- 进行上下文切换（context_switch），将新进程切换到运行状态
  - 保存老进程上下文
  - 执行地址空间切换（switch_mm_irqs_off）
  - 执行栈和寄存器切换（switch_to）

### 什么时候会触发调度/上下文切换？

- 时间片用完
- 网络/硬盘 IO（主动让出执行权）

### 保存进程上下文时，都保存到哪里？

### 任务切换都有哪些开销？

可以分为两种开销：直接开销和间接开销。

直接开销：切换过程中，必须要做的事情，例如地址空间、栈、寄存器切换等。
简介开销：缓存还不热，刚开始运行速度较慢。

### 进程上下文包含哪些内容？

包含地址空间、栈、寄存器等。

### 调度器相关的调度命令？

chrt：查看或修改进程的调度策略。详见[chrt 命令](../../../Commands/chrt.md)

```bash
sudo chrt -p 12345
```

nice：设置进程的 nice 值。nice 值越低占用越多 CPU。（和人类似，人越 nice 越倾向让出资源；越不 nice 越倾向抢夺资源）。详见[nice 命令](../../../Commands/nice.md)

```bash
nice -n -20 <PROGRAM_NAME>
```

renice：设置**正在运行**的进程的 nice 值。详见[renice 命令](../../../Commands/renice.md)

```bash
renice +5 <PID>
```

taskset：设置进程的调度亲缘性。详见[taskset 命令](../../../Commands/taskset.md)

## 草稿

CPU 调度是在**缓存性能**和**空闲核**这两个点之间做权衡。
